<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Kubernetes Networking: Part 2 - Calico | lbr.</title>

<meta name="description" content="Engineering, DevOps & Cloud Computing
">
<meta name="keywords" content="kubernetes, calico">
<link rel="canonical" href="/blog/2017/02/18/kubernetes-networking-calico.html">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-34865189-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-34865189-1');
</script>


<!--Feature assets-->
<!--Bootstrap JS-->
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>

<!--Font Awesome-->
<link rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.10.0/css/all.css" integrity="sha384-AYmEC3Yw5cVb3ZcuHtOA93w35dYTsvhLPVnYs9eStHfGJvOvKxVfELGroGkvsg+p" crossorigin="anonymous"/>

<!--Highlight.js-->
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.3.2/build/styles/default.min.css">
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.3.2/build/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!--Main assets-->
<link rel="icon" type="image/jpeg" href="/favicon.png"/>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<link rel="stylesheet" href="/assets/css/main.css">
</head>
<body>
<div class="wrapper">
<header class="header">
<div class="navigation">

<a href="/" class="logo">lbr.</a>

<ul class="menu">
<li class="menu__entry"><a href="/">about</a></li>
<li class="menu__entry"><a href="/projects">projects</a></li>
<li class="menu__entry"><a href="/blog">blog</a></li>
</ul>
</div>
<ul class="social-links">

<a href="mailto:/contact@leebriggs.co.uk" class="social-links__entry" target="_blank">
<i class="fas fa-envelope-square"></i>
</a>


<a href="https://github.com//jaxxstorm" class="social-links__entry" target="_blank">
<i class="fab fa-github"></i>
</a>


<a href="https://linkedin.com/in//briggsl" class="social-links__entry" target="_blank">
<i class="fab fa-linkedin-in"></i>
</a>

</ul>
</header>

<br>
<br>
<h1 class="post-title">
    <div class="post-title__text">Kubernetes Networking: Part 2 - Calico</div>
</h1>
<p class="post-title__subtitle">Published Feb 18, 2017  by <a href="https://leebriggs.co.uk/">Lee Briggs<a><br />


    <span class="badge badge-info">#kubernetes</span>

    <span class="badge badge-info">#calico</span>

<hr>
<br>
<p>In the previous post, I went over some basics of how Kubernetes networking works from a fundamental standpoint. The requirements are simple: every pod needs to have connectivity to every other pod. The only differentiation between the many options were how that was achieved.</p>

<p>In this post, I’m going to cover some of the fundamentals of how  <a href="https://www.projectcalico.org/">Calico</a> works. As I mentioned in the previous post, I really don’t like the idea that with these kubernetes deployments, you simply grab a yaml file and deploy it, sometimes with little to no explanation of what’s actually happening. Hopefully, this post will servce to better understand what’s going on.</p>

<p>As before, I’m not by any means a networking expert, so if you spot any mistakes, please <a href="https://github.com/jaxxstorm/jaxxstorm.github.io/pulls">send a pull request!</a></p>

<h1 id="what-is-calico">What is Calico?</h1>
<p>Calico is a container networking solution created by MetaSwitch. While solutions like Flannel operate over layer 2, Calico makes use of layer 3 to route packets to pods. The way it does this is relatively simple in practice.
Calico can also provide network policy for Kubernetes. We’ll ignore this for the time being, and focus purely on how it provides container networking.</p>

<h1 id="components">Components</h1>
<p>Your average calico setup has 4 components:</p>

<h2 id="etcd">Etcd</h2>

<p>Etcd is the backend data store for all the information Calico needs. If you’ve deployed Kubernetes already, you already <em>have</em> an etcd deployment, but it’s usually suggested to deploy a separate etcd for production systems, or at the very least deploy it outside of your kubernetes cluster.</p>

<p>You can examine the information that calico provides by using etcdctl. The default location for the calico keys is <code class="language-plaintext highlighter-rouge">/calico</code></p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>etcdctl <span class="nb">ls</span> /calico
/calico/ipam
/calico/v1
/calico/bgp</code></pre></figure>

<h2 id="bird">BIRD</h2>
<p>The next key component in the calico stack is <a href="http://bird.network.cz/">BIRD</a>. BIRD is a BGP routing daemon which runs on every host. Calico makes uses of BGP to propagate routes between hosts. BGP (if you’re not aware) is widely used to propagate routes over the internet. It’s suggested you make yourself familiar with some of the concepts if you’re using Calico.</p>

<p>Bird runs on every host in the Kubernetes cluster, usually as a <a href="https://kubernetes.io/docs/admin/daemons/">DaemonSet</a>. It’s included in the calico/node container.</p>

<h2 id="confd">Confd</h2>
<p><a href="https://github.com/kelseyhightower/confd">Confd</a> is a simple configuration management tool. It reads values from etcd and writes them to files on disk. If you take a look inside the calico/node container (where it usually runs) you can get an idea of what’s it doing:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># ps</span>
PID   USER     TIME   COMMAND
    1 root       0:00 /sbin/runsvdir <span class="nt">-P</span> /etc/service/enabled
  105 root       0:00 runsv felix
  106 root       0:00 runsv bird
  107 root       0:00 runsv bird6
  108 root       0:00 runsv confd
  109 root       0:28 bird6 <span class="nt">-R</span> <span class="nt">-s</span> /var/run/calico/bird6.ctl <span class="nt">-d</span> <span class="nt">-c</span> /etc/calico/confd/config/bird6.cfg
  110 root       0:00 confd <span class="nt">-confdir</span><span class="o">=</span>/etc/calico/confd <span class="nt">-interval</span><span class="o">=</span>5 <span class="nt">-watch</span> <span class="nt">--log-level</span><span class="o">=</span>debug <span class="nt">-node</span><span class="o">=</span>http://etcd1:4001
  112 root       0:40 bird <span class="nt">-R</span> <span class="nt">-s</span> /var/run/calico/bird.ctl <span class="nt">-d</span> <span class="nt">-c</span> /etc/calico/confd/config/bird.cfg
  230 root      31:48 calico-felix
  256 root       0:00 calico-iptables-plugin
  257 root       2:17 calico-iptables-plugin
11710 root       0:00 /bin/sh
11786 root       0:00 ps</code></pre></figure>

<p>As you can see, it’s connecting to the etcd nodes and reading from there, and it has a confd directory passed to it. The source of that confd directory can be found in the <a href="https://github.com/projectcalico/calico/tree/master/calico_node/filesystem/etc/calico/confd">calicoctl github repository</a>.</p>

<p>If you examine the repo, you’ll notice three directories.</p>

<p>Firstly, there’s a <code class="language-plaintext highlighter-rouge">conf.d</code> directory. This directory contains a bunch of toml configuration files. Let’s examine one of them:</p>

<figure class="highlight"><pre><code class="language-go" data-lang="go"><span class="p">[</span><span class="n">template</span><span class="p">]</span>
<span class="n">src</span> <span class="o">=</span> <span class="s">"bird_ipam.cfg.template"</span>
<span class="n">dest</span> <span class="o">=</span> <span class="s">"/etc/calico/confd/config/bird_ipam.cfg"</span>
<span class="n">prefix</span> <span class="o">=</span> <span class="s">"/calico/v1/ipam/v4"</span>
<span class="n">keys</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"/pool"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">reload_cmd</span> <span class="o">=</span> <span class="s">"pkill -HUP bird || true"</span></code></pre></figure>

<p>This is pretty simple in reality. It has a source file, and then where the file should be written to. Then, there’s some etcd keys that you should read information from. Essentially, confd is what writes the BIRD configuration for Calico. If you examine the keys there, you’ll see the kind of thing it reads:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span> etcdctl <span class="nb">ls</span> /calico/v1/ipam/v4/pool/
/calico/v1/ipam/v4/pool/192.168.0.0-16</code></pre></figure>

<p>So in this case, it’s getting the pod cidr we’ve assigned. I’ll cover this in more detail later.</p>

<p>In order to understand what it does with that key, you need to take a look at the <a href="https://github.com/projectcalico/calico/blob/master/calico_node/filesystem/etc/calico/confd/templates/bird_ipam.cfg.template">src template confd is using</a>.</p>

<p>Now, this at first glance looks a little complicated, but it’s not. It’s writing a file in the Go templating language that confd is familiar with. This is a standard BIRD configuration file, populated with keys from etcd. Take <a href="https://github.com/projectcalico/calico/blob/master/calico_node/filesystem/etc/calico/confd/templates/bird_ipam.cfg.template#L5-L8">this</a> for example:</p>

<p>This is essentially:</p>

<ul>
  <li>Looping through all the pools under the key <code class="language-plaintext highlighter-rouge">/v1/ipam/v4/pool</code> - in our case we only have one: 192.168.0.0-16</li>
  <li>Assigning the data in the pools key to a var, <code class="language-plaintext highlighter-rouge">$data</code></li>
  <li>Then grabbing a value from the JSON that’s been loaded into <code class="language-plaintext highlighter-rouge">$data</code> - in this case the cidr key.</li>
</ul>

<p>This makes more sense if you look at the values in the etcd key:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">etcdctl get /calico/v1/ipam/v4/pool/192.168.0.0-16
<span class="o">{</span><span class="s2">"cidr"</span>:<span class="s2">"192.168.0.0/16"</span>,<span class="s2">"ipip"</span>:<span class="s2">"tunl0"</span>,<span class="s2">"masquerade"</span>:true,<span class="s2">"ipam"</span>:true,<span class="s2">"disabled"</span>:false<span class="o">}</span></code></pre></figure>

<p>So it’s grabbed the cidr value and written it to the file. The end result of the file in the calico/node container brings this all together:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="k">if</span> <span class="o">(</span> net ~ 192.168.0.0/16 <span class="o">)</span> <span class="k">then</span> <span class="o">{</span>
    accept<span class="p">;</span>
  <span class="o">}</span></code></pre></figure>

<p>Pretty simple really!</p>

<h2 id="calico-felix">calico-felix</h2>
<p>The final component in the calico stack is the calico-felix daemon. This is the tool that performs all the magic in the calico stack. It has multiple responsibilities:</p>

<ul>
  <li>it writes the routing table of the operating system. You’ll see this in action later</li>
  <li>it manipulates IPtables on the host. Again, you’ll see this in action later.</li>
</ul>

<p>It does all this by connecting to etcd and reading information from there. It runs inside the calico/node DaemonSet alongside confd and BIRD.</p>

<h1 id="calico-in-action">Calico in Action</h1>
<p>In order to get started, it’s recommend that you’ve deployed Calico using the installation instructions <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/">here</a>. Ensure that:</p>

<ul>
  <li>you’ve got a calico/node container running on every kubernetes host</li>
  <li>You can see in the calico/node logs that there’s no errors or issues. Use <code class="language-plaintext highlighter-rouge">kubectl get logs</code> on a few hosts to ensure it’s working as expected</li>
</ul>

<p>At this stage, you’ll want to deploy something so that Calico can work it’s magic. I recommend deploying the <a href="https://github.com/kubernetes/kubernetes/tree/master/examples/guestbook/all-in-one">guestbook</a> to see all this in action.</p>

<h2 id="routing-table">Routing Table</h2>
<p>Once you’ve deployed Calico and your guestbook, get the pod IP of the guestbook using <code class="language-plaintext highlighter-rouge">kubectl</code>:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">kubectl get po <span class="nt">-o</span> wide
NAME                           READY     STATUS    RESTARTS   AGE       IP                NODE
frontend-88237173-f3sz4        1/1       Running   0          2m        192.168.15.195    node1
frontend-88237173-j407q        1/1       Running   0          2m        192.168.228.195   node2
frontend-88237173-pwqfx        1/1       Running   0          2m        192.168.175.195   node3
redis-master-343230949-zr5xg   1/1       Running   0          2m        192.168.0.130    node4
redis-slave-132015689-475lt    1/1       Running   0          2m        192.168.71.1      node5
redis-slave-132015689-dzpks    1/1       Running   0          2m        192.168.105.65   node6</code></pre></figure>

<p>If everything has worked correctly, you should be able to ping every pod from any host. Test this now:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ping <span class="nt">-c</span> 1 192.168.15.195
PING 192.168.15.195 <span class="o">(</span>192.168.15.195<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.15.195: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>63 <span class="nb">time</span><span class="o">=</span>0.318 ms

<span class="nt">---</span> 192.168.15.195 ping statistics <span class="nt">---</span>
1 packets transmitted, 1 received, 0% packet loss, <span class="nb">time </span>0ms
rtt min/avg/max/mdev <span class="o">=</span> 0.318/0.318/0.318/0.000 ms</code></pre></figure>

<p>If you have <a href="http://fping.org/">fping</a> and  installed, you can verify all pods in one go:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">kubectl get po <span class="nt">-o</span> json | jq .items[].status.podIP <span class="nt">-r</span> | fping
192.168.15.195 is alive
192.168.228.195 is alive
192.168.175.195 is alive
192.168.0.130 is alive
192.168.71.1 is alive
192.168.105.65 is alive</code></pre></figure>

<p>The real question is, how did this actually work? How come I can ping these endpoints? The answer becomes obvious if you print the routing table:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ip route
default via 172.29.132.1 dev eth0
169.254.0.0/16 dev eth0  scope <span class="nb">link  </span>metric 1002
172.17.0.0/16 dev docker0  proto kernel  scope <span class="nb">link  </span>src 172.17.0.1
172.29.132.0/24 dev eth0  proto kernel  scope <span class="nb">link  </span>src 172.29.132.127
172.29.132.1 dev eth0  scope <span class="nb">link
</span>192.168.0.128/26 via 172.29.141.98 dev tunl0  proto bird onlink
192.168.15.192/26 via 172.29.141.95 dev tunl0  proto bird onlink
blackhole 192.168.33.0/26  proto bird
192.168.71.0/26 via 172.29.141.105 dev tunl0  proto bird onlink
192.168.105.64/26 via 172.29.141.97 dev tunl0  proto bird onlink
192.168.175.192/26 via 172.29.141.102 dev tunl0  proto bird onlink
192.168.228.192/26 via 172.29.141.96 dev tunl0  proto bird onlink</code></pre></figure>

<p>A lot has happened here, so let’s break it down in sections.</p>

<h3 id="subnets">Subnets</h3>

<p>Each host that has calico/node running on it has its own <code class="language-plaintext highlighter-rouge">/26</code> subnet. You can verify this by looking in etcd:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">etcdctl <span class="nb">ls</span> /calico/ipam/v2/host/node1/ipv4/block/
/calico/ipam/v2/host/node1/ipv4/block/192.168.228.192-26</code></pre></figure>

<p>So in this case, the host node1 has been allocated the subnet <code class="language-plaintext highlighter-rouge">192.168.228.192-26</code>. Any new host that starts up, connects to kubernetes and has a calico/node container running on it, will get one of those subnets. This is a fairly standard model in Kubernetes networking.</p>

<p>What differs here is how Calico handles it. Let’s go back to our routing table and look at the entry for that subnet:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">192.168.228.192/26 via 172.29.141.96 dev tunl0  proto bird onlink</code></pre></figure>

<p>What’s happened here is that calico-felix has read etcd, and determined that the ip address of node1 is <code class="language-plaintext highlighter-rouge">172.29.141.96</code>. Calico now knows the IP address of the host, and also the pod subnet assigned to it. With this information, it programs routes on <em>every node</em> in the kubernetes cluster. It says “if you want to hit something in this subnet, go via the ip address <code class="language-plaintext highlighter-rouge">x</code> over the tunl0 interface.</p>

<p>The tunl0 interface <em>may not</em> be present on your host. It exists here because I’ve enabled IPIP encapsulation in Calico for the sake of testing.</p>

<h3 id="destination-host">Destination Host</h3>

<p>Now, the packets know their destination. They have a route defined and they know they should head directly via the interface of the node. What happens then, when they arrive there?</p>

<p>The answer again, is in the routing table. On the host the pod has been scheduled on, print the routing table again:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ip route
default via 172.29.132.1 dev eth0
169.254.0.0/16 dev eth0  scope <span class="nb">link  </span>metric 1002
172.17.0.0/16 dev docker0  proto kernel  scope <span class="nb">link  </span>src 172.17.0.1
172.29.132.0/24 dev eth0  proto kernel  scope <span class="nb">link  </span>src 172.29.132.127
172.29.132.1 dev eth0  scope <span class="nb">link
</span>192.168.0.128/26 via 172.29.141.98 dev tunl0  proto bird onlink
192.168.15.192/26 via 172.29.141.95 dev tunl0  proto bird onlink
blackhole 192.168.33.0/26  proto bird
192.168.71.0/26 via 172.29.141.105 dev tunl0  proto bird onlink
192.168.105.64/26 via 172.29.141.97 dev tunl0  proto bird onlink
192.168.175.192/26 via 172.29.141.102 dev tunl0  proto bird onlink
192.168.228.192/26 via 172.29.141.96 dev tunl0  proto bird onlink
192.168.228.195 dev cali7b262072819  scope <span class="nb">link</span></code></pre></figure>

<p>There’s an extra route! You can see, there’s the pod IP has the destination and it’s telling the OS to route it via a device, <code class="language-plaintext highlighter-rouge">cali7b262072819</code>.</p>

<p>Let’s have a look at the interfaces:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP mode DEFAULT qlen 1000
    <span class="nb">link</span>/ether 00:25:90:62:ed:c6 brd ff:ff:ff:ff:ff:ff
4: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT
    <span class="nb">link</span>/ether 00:25:90:62:ed:c6 brd ff:ff:ff:ff:ff:ff
5: cali7b262072819@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT
    <span class="nb">link</span>/ether 32:e9:d2:f3:17:0f brd ff:ff:ff:ff:ff:ff link-netnsid 4</code></pre></figure>

<p>There’s an interface for our pod! When the container spun up, calico (via <a href="https://github.com/containernetworking/cni">CNI</a>) created an interface for us and assigned it to the pod. How did it do that?</p>

<h2 id="cni">CNI</h2>

<p>The answer lies in the setup of Calico. If you examine the yaml you installed when you installed Calico, you’ll see a setup task which runs on every container. That uses a configmap, which looks like this</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="c1"># This ConfigMap is used to configure a self-hosted Calico installation.</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">calico-config</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="c1"># The location of your etcd cluster.  This uses the Service clusterIP</span>
  <span class="c1"># defined below.</span>
  <span class="na">etcd_endpoints</span><span class="pi">:</span> <span class="s2">"</span><span class="s">http://10.96.232.136:6666"</span>

  <span class="c1"># True enables BGP networking, false tells Calico to enforce</span>
  <span class="c1"># policy only, using native networking.</span>
  <span class="na">enable_bgp</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>

  <span class="c1"># The CNI network configuration to install on each node.</span>
  <span class="na">cni_network_config</span><span class="pi">:</span> <span class="pi">|-</span>
    <span class="s">{</span>
        <span class="s">"name": "k8s-pod-network",</span>
        <span class="s">"type": "calico",</span>
        <span class="s">"etcd_endpoints": "__ETCD_ENDPOINTS__",</span>
        <span class="s">"log_level": "info",</span>
        <span class="s">"ipam": {</span>
            <span class="s">"type": "calico-ipam"</span>
        <span class="s">},</span>
        <span class="s">"policy": {</span>
            <span class="s">"type": "k8s",</span>
             <span class="s">"k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",</span>
             <span class="s">"k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"</span>
        <span class="s">},</span>
        <span class="s">"kubernetes": {</span>
            <span class="s">"kubeconfig": "/etc/cni/net.d/__KUBECONFIG_FILENAME__"</span>
        <span class="s">}</span>
    <span class="s">}</span>

  <span class="c1"># The default IP Pool to be created for the cluster.</span>
  <span class="c1"># Pod IP addresses will be assigned from this pool.</span>
  <span class="s">ippool.yaml</span><span class="pi">:</span> <span class="pi">|</span>
      <span class="s">apiVersion: v1</span>
      <span class="s">kind: ipPool</span>
      <span class="s">metadata:</span>
        <span class="s">cidr: 192.168.0.0/16</span>
      <span class="s">spec:</span>
        <span class="s">ipip:</span>
          <span class="s">enabled: true</span>
        <span class="s">nat-outgoing: true</span></code></pre></figure>

<p>This manifests itself in the <code class="language-plaintext highlighter-rouge">/etc/cni/net.d</code> directory on every host:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">ls</span> /etc/cni/net.d/
10-calico.conf  calico-kubeconfig  calico-tls</code></pre></figure>

<p>So essentially, when a new pod starts up, Calico will:</p>

<ul>
  <li>query the kubernetes API to determine the pod exists and that it’s on this node</li>
  <li>assigns the pod an IP address from within its IPAM</li>
  <li>create an interface on the host so that the container can get an address</li>
  <li>tell the kubernetes API about this new IP</li>
</ul>

<p>Magic!</p>

<h2 id="iptables">IPTables</h2>

<p>The final piece of the puzzle here is some IPTables magic. As mentioned earlier, Calico has support for network policy. Even if you’re not actively <em>using</em> the policy components, it still exists, and you need some default policy for connectivity is work. If you look at the output of <code class="language-plaintext highlighter-rouge">iptables -L</code> you’ll see a familiar string:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="k">**</span>Chain felix-to-7b262072819 <span class="o">(</span>1 references<span class="o">)</span>
target     prot opt <span class="nb">source               </span>destination
MARK       all  <span class="nt">--</span>  anywhere             anywhere             MARK and 0xfeffffff
MARK       all  <span class="nt">--</span>  anywhere             anywhere             /<span class="k">*</span> Start of tier default <span class="k">*</span>/ MARK and 0xfdffffff
felix-p-_722590149132d26-i  all  <span class="nt">--</span>  anywhere             anywhere             mark match 0x0/0x2000000
RETURN     all  <span class="nt">--</span>  anywhere             anywhere             mark match 0x1000000/0x1000000 /<span class="k">*</span> Return <span class="k">if </span>policy accepted <span class="k">*</span>/
DROP       all  <span class="nt">--</span>  anywhere             anywhere             mark match 0x0/0x2000000 /<span class="k">*</span> Drop <span class="k">if </span>no policy <span class="k">in </span>tier passed <span class="k">*</span>/
felix-p-k8s_ns.default-i  all  <span class="nt">--</span>  anywhere             anywhere
RETURN     all  <span class="nt">--</span>  anywhere             anywhere             mark match 0x1000000/0x1000000 /<span class="k">*</span> Profile accepted packet <span class="k">*</span>/
DROP       all  <span class="nt">--</span>  anywhere             anywhere             /<span class="k">*</span> Packet did not match any profile <span class="o">(</span>endpoint eth0<span class="o">)</span> <span class="k">*</span>/</code></pre></figure>

<p>The IPtables chain here has the same string at the calico interface. This iptables rule is vital for calico to pass the packets onto the container. It grabs the packet destined for the container, determines if it should be allowed and sends it on its way if it is.</p>

<p>If this chain doesn’t exist, it gets captured by the default policy, and the packet will be dropped. It’s <code class="language-plaintext highlighter-rouge">calico-felix</code> that programs these rules.</p>

<h1 id="wrap-up">Wrap Up</h1>

<p>Hopefully, you now have a better knowledge of how exactly Calico gets the job done. At its core, it’s actually relatively simple, simply ip routes on each host. What it does it take the difficult in managing those routes away from you, giving you a simple, easy solution to container networking.</p>


<br>

<div id="disqus_thread"></div>
<script>


var disqus_config = function () {
this.page.url = 'http://localhost:4000/assets/style.css';
this.page.identifier = '/blog/2017/02/18/kubernetes-networking-calico.html'; 
};

(function() {
var d = document, s = d.createElement('script');
s.src = 'https://.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<br>
<div class="about">
<div class="about__devider">*****</div>
<div class="about__text">
<br>
&#169 2021, Lee Briggs | <a href="https://github.com/ritijjain/pudhina-fresh">Pudhina Fresh</a> theme for Jekyll.
</div>
</div>

</div>
</body>
</html>
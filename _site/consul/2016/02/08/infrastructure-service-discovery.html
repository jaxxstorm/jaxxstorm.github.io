<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Infrastructure Service Discovery with Consul | lbr.</title>

<meta name="description" content="Engineering, DevOps & Cloud Computing
">
<meta name="keywords" content="">
<link rel="canonical" href="/consul/2016/02/08/infrastructure-service-discovery.html">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-34865189-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-34865189-1');
</script>


<!--Feature assets-->
<!--Bootstrap JS-->
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>

<!--Font Awesome-->
<link rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.10.0/css/all.css" integrity="sha384-AYmEC3Yw5cVb3ZcuHtOA93w35dYTsvhLPVnYs9eStHfGJvOvKxVfELGroGkvsg+p" crossorigin="anonymous"/>

<!--Highlight.js-->
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.3.2/build/styles/default.min.css">
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.3.2/build/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!--Main assets-->
<link rel="icon" type="image/jpeg" href="/favicon.png"/>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<link rel="stylesheet" href="/assets/css/main.css">
</head>
<body>
<div class="wrapper">
<header class="header">
<div class="navigation">

<a href="/" class="logo">lbr.</a>

<ul class="menu">
<li class="menu__entry"><a href="/">about</a></li>
<li class="menu__entry"><a href="/projects">projects</a></li>
<li class="menu__entry"><a href="/blog">blog</a></li>
</ul>
</div>
<ul class="social-links">

<a href="mailto:/contact@leebriggs.co.uk" class="social-links__entry" target="_blank">
<i class="fas fa-envelope-square"></i>
</a>


<a href="https://github.com//jaxxstorm" class="social-links__entry" target="_blank">
<i class="fab fa-github"></i>
</a>


<a href="https://linkedin.com/in//briggsl" class="social-links__entry" target="_blank">
<i class="fab fa-linkedin-in"></i>
</a>

</ul>
</header>

<br>
<br>
<h1 class="post-title">
    <div class="post-title__text">Infrastructure Service Discovery with Consul</div>
</h1>
<p class="post-title__subtitle">Published Feb  8, 2016  by <a href="https://leebriggs.co.uk/">Lee Briggs<a><br />


<hr>
<br>
<p>I had a problem recently. I’m deploying services, and everything is Puppetized, but I have to manually tell other infrastructure that it exists. It’s frustrating. As an “ops guy” I focus on making my infrastructure services available, resiliant and distributed so that they can scale well and not fail catastrophically. I think we’ve all done this when deploying $things, and most people (in my experience) go through the following stages..</p>

<h3 id="stage-1---dns-based-discovery">Stage 1 - DNS based discovery</h3>

<p>Everyone has used or is using a poor man’s load balancer somewhere in their infrastructure. DNS is also basically the most basic of service discovery tools, you enter a DNS name and it provides the address of the service! Great! You can also get really fancy and use SRV records for port discovery as well, but then you realise there’s quite a few problems with doing load balancing and service discovery like this:</p>

<ul>
  <li>One of the servers in your infrastructure breaks or goes offline.</li>
  <li>The DNS record (either an A record or SRV record) for broken service still exists</li>
  <li>Your DNS server, quite rightly, keeps resolving that address for $service because it doesn’t know any better</li>
  <li>Pretty much half of your requests to $service fail (assuming you have two servers for $service)</li>
</ul>

<p>I know this pain, and I’ve had to deal with an outage like this. In order to fix this problem, I went with stage 2..</p>

<h3 id="stage-2---an-actual-load-balancer">Stage 2 - An actual load balancer</h3>

<p>Once you have your service fail once (and it will..) you think you need a better solution, so you look at an actual load balancer, like HAProxy (or in my case, an F5 Big-IP). This has extra goodness, like service availability healthchecks, and will present a single VIP for the service address. You add your service to the load balancing pool, set up a healthcheck and assign a VIP to it, and it will yank out any service provider that isn’t performing as expected (perhaps the TCP port doesn’t respond?) - Voila! You’re not going to have failures for $service now.</p>

<p>This is really great for us infrastructure guys, and a lot of people stop there. Their service is now reliable, and all you have to do is set up a DNS record for the VIP and point all your clients to it.</p>

<p>Well, this wasn’t good enough for me because everytime I provisioned a new instance of $service, I had to add it to the load balancer pool. Initially we did it manually, and then we got bored and used the API. I was still annoyed though, because I had to keep track of what $service was running where and make sure every instance of it was in the pool. In a land managed by configuration management, this really wasn’t much fun at all. I want to provision a VM for $service, and I want it to identify when it’s ready and start serving traffic automatically, with no manual intervention required.</p>

<p>The straw that broke the camels back for me was spinning up a new Puppetmaster. We might do this rarely, but it should be an automated job - create a new VM and assign it the Puppetmaster role in Puppet, then use a little script on VM startup to add the puppetmaster to the load balancing pool. It worked, but I wanted more.</p>

<ul>
  <li>Notifications when a puppetmaster failed, so I could fix</li>
  <li>Service availability announcements - when the Puppetmaster was ready, I wanted it to announce its availability to the world and start serving traffic. A script just didn’t feel…right.</li>
</ul>

<p>This is how I got to stage 3 - with service discovery. <a href="https://www.consul.io/">Consul</a>, a service discovery tool written by <a href="https://www.hashicorp.com/">hashicorp</a> was the key.</p>

<h3 id="stage-3---a-different-way">Stage 3 - A different way</h3>

<p>Before I get started, I must note that there are many other tools in this space. Things like <a href="http://nerds.airbnb.com/smartstack-service-discovery-cloud/">SmartStack</a> and <a href="https://zookeeper.apache.org/">Zookeeper</a> can do things like this for you, but I went with <a href="https://www.consul.io/">Consul</a> for a few reasons:</p>

<ul>
  <li>It uses operationally available tools to practice service discovery, like DNS and Nagios Checks</li>
  <li>It’s written in Go (performance, concurrency, language agnostic)</li>
  <li>We use hashicorp tools elsewhere, and they have always proved to be very reliable and well designed.</li>
</ul>

<p>In order for consul to do its thing, you need to understand a few basic concepts about how it works..</p>

<ul>
  <li>The best way to implement is to deploy the consul agent to all your service providing infrastructure and clients. The way consul works means that this seems (to me) to be the best implementation.</li>
  <li>The consul agent will form a cluster using the <a href="http://thesecretlivesofdata.com/raft">raft consensus protocol</a></li>
  <li>There are some agents in your infrastructure that operate in “server mode” - these perform consensus checks and elect a leader. You need to decide how many there are in advance, and I suggest an odd number so they can elect a leader.</li>
  <li>Consul uses DNS for service discovery. To do that, it provides a DNS resolver on port 8600.</li>
  <li>The way it decides what to serve on that DNS resolver is by healthchecking services and determining their health status. If the service is healthy, you can query port 8600 on <em>any</em> consul agent (port 8600) and it will provide the list of available servers.</li>
  <li>The healthcheck can be done in a variety of ways, but a particularly nice way of doing it is by having consil execute nagios scripts. There’s also a pure TCP check, a HTTP check and many more</li>
</ul>

<p>This provides three interesting problems for deployment:</p>

<ul>
  <li>How do I get my DNS queries to Consul?</li>
  <li>How do I deploy Consul?</li>
  <li>How do I put an infrastructure service in Consul?</li>
</ul>

<p>Well, I’m going to write about the ways I did this!</p>

<h2 id="deploying-consul-with-puppet">Deploying Consul with Puppet</h2>

<p>Consul has an excellent <a href="https://github.com/solarkennedy/puppet-consul">Puppet Module</a> written by <a href="https://github.com/solarkennedy">@solarkennedy</a> of Yelp which will handle the deploy for you pretty nicely. I found that it wasn’t great at <em>bootstrapping</em> the cluster, but once you have your servers up and running it worked pretty flawlessly!</p>

<h3 id="deploying-the-servers">Deploying the servers</h3>

<p>To deploy the servers with Puppet, create a consul server role and include the puppet module:</p>

<figure class="highlight"><pre><code class="language-puppet" data-lang="puppet"><span class="k">node</span> <span class="s1">'consulserver'</span> <span class="p">{</span>
  <span class="k">class</span> <span class="p">{</span> <span class="s1">'::consul'</span><span class="p">:</span>
    <span class="py">config_hash</span> <span class="p">=&gt;</span> <span class="p">{</span>
      <span class="py">datacenter</span>       <span class="p">=&gt;</span> <span class="s2">"home"</span><span class="p">,</span>
      <span class="py">client_addr</span>      <span class="p">=&gt;</span> <span class="s2">"0.0.0.0"</span><span class="p">,</span> <span class="c"># ensures the server is listening on a public interface
</span>      <span class="py">node_name</span>        <span class="p">=&gt;</span> <span class="nv">$::fqdn</span><span class="p">,</span>
      <span class="py">bootstrap_expect</span> <span class="p">=&gt;</span> <span class="m">3</span><span class="p">,</span> <span class="c"># the number of servers that should be found before attempting to create a consul cluster
</span>      <span class="py">server</span>           <span class="p">=&gt;</span> <span class="kc">true</span><span class="p">,</span>
      <span class="py">data_dir</span>         <span class="p">=&gt;</span> <span class="s2">"/opt/consul"</span><span class="p">,</span>
      <span class="py">ui_dir</span>           <span class="p">=&gt;</span> <span class="s2">"/opt/consul/ui"</span><span class="p">,</span>
      <span class="py">recusors</span>         <span class="p">=&gt;</span> <span class="p">[</span><span class="s1">'8.8.8.8'</span><span class="p">,</span> <span class="s1">'192.168.0.1'</span><span class="p">],</span> <span class="c"># Your upstream DNS servers
</span>    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>The important params here are:</p>
<ul>
  <li>bootstrap_expect: How large should your server cluster be?</li>
  <li>node_name: Make sure it’s unique, $::fqdn fact seems reasonable to me</li>
  <li>server: true - make sure it’s a server</li>
</ul>

<p>Once you’ve deployed this to your three consul hosts, and the service is started, you’ll see something like this in the logs of each server:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.</code></pre></figure>

<p>What’s happening here is that your cluster is looking for peers, but it can’t find them, so let’s make a cluster. From <em>one</em> of the servers, perform the following command:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>consul <span class="nb">join</span> &lt;Node A Address&gt; &lt;Node B Address&gt; &lt;Node C Address&gt;
Successfully joined cluster by contacting 3 nodes.</code></pre></figure>

<p>and then, in the logs, you’ll see something like this</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>INFO] consul: adding server foo <span class="o">(</span>Addr: 127.0.0.2:8300<span class="o">)</span> <span class="o">(</span>DC: dc1<span class="o">)</span>
<span class="o">[</span>INFO] consul: adding server bar <span class="o">(</span>Addr: 127.0.0.1:8300<span class="o">)</span> <span class="o">(</span>DC: dc1<span class="o">)</span>
<span class="o">[</span>INFO] consul: Attempting bootstrap with nodes: <span class="o">[</span>127.0.0.3:8300 127.0.0.2:8300 127.0.0.1:8300]
...
<span class="o">[</span>INFO] consul: cluster leadership acquired</code></pre></figure>

<p>You have now bootstrapped a consul cluster, and you’re ready to start adding agents to it from the rest of your infrastructure!</p>

<h3 id="deploying-the-agents">Deploying the agents</h3>

<p>As I said earlier, you’ll probably want to deploy the agent to every single host that hosts a service or queries a service. There are other ways to do this, such as not deploying the agent everywhere and changing your DNS servers to resolve to the consul servers, but I chose to do it this way. Your mileage may vary.</p>

<p>Using Puppet, you deploy the agent to every server like so:</p>

<figure class="highlight"><pre><code class="language-puppet" data-lang="puppet"><span class="k">node</span> <span class="s1">'default'</span> <span class="p">{</span>
  <span class="k">class</span> <span class="p">{</span> <span class="s1">'::consul'</span><span class="p">:</span>
    <span class="py">datacenter</span>    <span class="p">=&gt;</span> <span class="s2">"home"</span><span class="p">,</span>
    <span class="py">client_addr</span>   <span class="p">=&gt;</span> <span class="s2">"0.0.0.0"</span><span class="p">,</span>
    <span class="py">node_name</span>     <span class="p">=&gt;</span> <span class="nv">$::fqdn</span><span class="p">,</span>
    <span class="py">data_dir</span>      <span class="p">=&gt;</span> <span class="s2">"/opt/consul"</span><span class="p">,</span>
    <span class="py">retry_join</span>    <span class="p">=&gt;</span> <span class="p">[</span><span class="s2">"server_a"</span><span class="p">,</span> <span class="s2">"server_b"</span><span class="p">,</span> <span class="s2">"server_c"</span><span class="p">],</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>The key differences from the servers are:</p>

<ul>
  <li>the server param is not defined (it’s false by default)</li>
  <li>retry_join is set: this tells the agent to try and retry these servers and therefore rejoin the cluster when it starts up.</li>
</ul>

<p>Once you’ve deployed this, you’ll have a consul cluster running with agents attached. You can see the status of the cluster like so:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@host~]# consul members
Node          Address            Status  Type    Build  Protocol  DC
hostD         192.168.4.26:8301  alive   client  0.6.3  2         home
hostA         192.168.4.21:8301  alive   server  0.6.3  2         home
hostB         192.168.4.29:8301  alive   server  0.6.3  2         home
hostC         192.168.4.34:8301  alive   server  0.6.3  2         home</code></pre></figure>

<h2 id="consul-services">Consul Services</h2>

<p>Now we have our cluster deployed, we need to make it aware of services. There’s a service already deployed for the consul cluster itself, and you can see how it’s deployed and the status of it using a DNS query to the consul agent.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">dig +short @127.0.0.1 <span class="nt">-p</span> 8600 consul.service.home.consul. ANY
192.168.4.34
192.168.4.21
192.168.4.29</code></pre></figure>

<p>Here, consul has returned the status of the consul service to let me know it’s available from these IP addresses. Consul also supports SRV records, so it can even return the port that it’s listening on</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">dig +short @127.0.0.1 <span class="nt">-p</span> 8600 consul.service.home.consul. SRV
1 1 8300 nodeA.node.home.consul.
1 1 8300 nodeB.node.home.consul.
1 1 8300 nodeC.node.home.consul.</code></pre></figure>

<p>The way it determines what nodes are available to provide a service is using <em>checks</em> which I mentioned earlier. These can be either:</p>

<ul>
  <li>A script which is executed and returns a nagios compliant code, where 0 is healthy and anything else is an error</li>
  <li>HTTP check which returns a HTTP response code, where anything with 2XX is healthy</li>
  <li>TCP check, basically checking if a port is open.</li>
</ul>

<p>There are 2 more, TTL and Docker + Interval, but for the sake of this post I’m going to refer you to the <a href="https://www.consul.io/docs/agent/checks.html">documentation</a> for those.</p>

<p>In order for us to get started with a consul service, we need to deploy a check..</p>

<h2 id="puppetmaster-service">Puppetmaster service</h2>

<p>I chose to first deploy a puppetmaster service check, so I’ll use that as my example. Again, I used the <a href="https://github.com/solarkennedy/puppet-consul">puppet module</a> to do this, so in my Puppetmaster role definition, I simple did this:</p>

<figure class="highlight"><pre><code class="language-puppet" data-lang="puppet"><span class="k">node</span> <span class="s1">'puppetmaster'</span> <span class="p">{</span>
  <span class="p">::</span><span class="n">consul::service</span> <span class="p">{</span> <span class="s1">'puppetmaster'</span><span class="p">:</span>
    <span class="py">port</span> <span class="p">=&gt;</span> <span class="s1">'8140'</span><span class="p">,</span>
    <span class="py">tags</span> <span class="p">=&gt;</span> <span class="p">[</span><span class="s1">'puppet'</span><span class="p">],</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>This defines the service that this node provides and on which port. I now need to define the healthcheck for this service - I used a simple TCP check:</p>

<figure class="highlight"><pre><code class="language-puppet" data-lang="puppet"><span class="p">::</span><span class="n">consul::check</span> <span class="p">{</span> <span class="s1">'puppetmaster_tcp'</span><span class="p">:</span>
    <span class="py">interval</span>   <span class="p">=&gt;</span> <span class="s1">'60'</span><span class="p">,</span>
    <span class="py">tcp</span>        <span class="p">=&gt;</span> <span class="s1">'localhost:8140'</span><span class="p">,</span>
    <span class="py">notes</span>      <span class="p">=&gt;</span> <span class="s1">'Puppetmasters listen on port 8140'</span><span class="p">,</span>
    <span class="py">service_id</span> <span class="p">=&gt;</span> <span class="s1">'puppetmaster'</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>

<p>now, when Puppet converges, I should be able to query my service on the Puppetmaster:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">dig +short @127.0.0.1 <span class="nt">-p</span> 8600 puppetmaster.service.home.consul. SRV
1 1 8140 puppetmaster.example.lan.node.home.consul.</code></pre></figure>

<p>Excellent, the service exists and it must be healthy, because there’s a result for the service. Just to confirm this, lets use consul’s <a href="https://www.consul.io/docs/agent/http.html">http API</a> to query the service status:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@puppetmaster ~]# curl <span class="nt">-s</span> http://127.0.0.1:8500/v1/health/service/puppetmaster | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"Node"</span>: <span class="o">{</span>
      <span class="s2">"Node"</span>: <span class="s2">"puppetmaster.example.lan"</span>,
      <span class="s2">"Address"</span>: <span class="s2">"192.168.4.21"</span>,
      <span class="s2">"CreateIndex"</span>: 5,
      <span class="s2">"ModifyIndex"</span>: 11154
    <span class="o">}</span>,
    <span class="s2">"Service"</span>: <span class="o">{</span>
      <span class="s2">"ID"</span>: <span class="s2">"puppetmaster"</span>,
      <span class="s2">"Service"</span>: <span class="s2">"puppetmaster"</span>,
      <span class="s2">"Tags"</span>: <span class="o">[</span>
        <span class="s2">"puppet"</span>
      <span class="o">]</span>,
      <span class="s2">"Address"</span>: <span class="s2">""</span>,
      <span class="s2">"Port"</span>: 8140,
      <span class="s2">"EnableTagOverride"</span>: <span class="nb">false</span>,
      <span class="s2">"CreateIndex"</span>: 5535,
      <span class="s2">"ModifyIndex"</span>: 5877
    <span class="o">}</span>,
    <span class="s2">"Checks"</span>: <span class="o">[</span>
      <span class="o">{</span>
        <span class="s2">"Node"</span>: <span class="s2">"puppetmaster.example.lan"</span>,
        <span class="s2">"CheckID"</span>: <span class="s2">"puppetmaster_tcp"</span>,
        <span class="s2">"Name"</span>: <span class="s2">"puppetmaster_tcp"</span>,
        <span class="s2">"Status"</span>: <span class="s2">"passing"</span>,
        <span class="s2">"Notes"</span>: <span class="s2">"Puppetmasters listen on port 8140"</span>,
        <span class="s2">"Output"</span>: <span class="s2">"TCP connect localhost:8140: Success"</span>,
        <span class="s2">"ServiceID"</span>: <span class="s2">"puppetmaster"</span>,
        <span class="s2">"ServiceName"</span>: <span class="s2">"puppetmaster"</span>,
        <span class="s2">"CreateIndex"</span>: 5601,
        <span class="s2">"ModifyIndex"</span>: 5877
      <span class="o">}</span>,
      <span class="o">{</span>
        <span class="s2">"Node"</span>: <span class="s2">"puppetmaster.example.lan"</span>,
        <span class="s2">"CheckID"</span>: <span class="s2">"serfHealth"</span>,
        <span class="s2">"Name"</span>: <span class="s2">"Serf Health Status"</span>,
        <span class="s2">"Status"</span>: <span class="s2">"passing"</span>,
        <span class="s2">"Notes"</span>: <span class="s2">""</span>,
        <span class="s2">"Output"</span>: <span class="s2">"Agent alive and reachable"</span>,
        <span class="s2">"ServiceID"</span>: <span class="s2">""</span>,
        <span class="s2">"ServiceName"</span>: <span class="s2">""</span>,
        <span class="s2">"CreateIndex"</span>: 5,
        <span class="s2">"ModifyIndex"</span>: 11150
      <span class="o">}</span>
    <span class="o">]</span>
  <span class="o">}</span>
<span class="o">]</span></code></pre></figure>

<h3 id="a-failing-check">A failing check</h3>

<p>Now, this is great at this point, we have a healthy service with a passing healthcheck - but what happens when something breaks. Let’s say a Puppetmaster service is stopped - what exactly happens?</p>

<p>Well, let’s stop our Puppetmaster and see.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@puppetmaster ~]# service httpd stop
Redirecting to /bin/systemctl stop  httpd.service <span class="c"># I use passenger to serve puppetmasters, so we'll stop http</span></code></pre></figure>

<p>Now, let’s do our DNS query again</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@puppetmaster ~]# dig +short @127.0.0.1 <span class="nt">-p</span> 8600 puppetmaster.service.home.consul. SRV
<span class="o">[</span>root@puppetmaster ~]#</code></pre></figure>

<p>I’m not getting <em>any</em> dns results from consul. This is basically because I’ve only deployed one Puppetmaster, and I just stopped it from running, but in a multi-node setup, it would return only the healthy nodes. I can confirm this from the consul API again:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@puppetmaster ~]# curl <span class="nt">-s</span> http://127.0.0.1:8500/v1/health/service/puppetmaster | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"Node"</span>: <span class="o">{</span>
      <span class="s2">"Node"</span>: <span class="s2">"puppetmaster.example.lan"</span>,
      <span class="s2">"Address"</span>: <span class="s2">"192.168.4.21"</span>,
      <span class="s2">"CreateIndex"</span>: 5,
      <span class="s2">"ModifyIndex"</span>: 97009
    <span class="o">}</span>,
    <span class="s2">"Service"</span>: <span class="o">{</span>
      <span class="s2">"ID"</span>: <span class="s2">"puppetmaster"</span>,
      <span class="s2">"Service"</span>: <span class="s2">"puppetmaster"</span>,
      <span class="s2">"Tags"</span>: <span class="o">[</span>
        <span class="s2">"puppet"</span>
      <span class="o">]</span>,
      <span class="s2">"Address"</span>: <span class="s2">""</span>,
      <span class="s2">"Port"</span>: 8140,
      <span class="s2">"EnableTagOverride"</span>: <span class="nb">false</span>,
      <span class="s2">"CreateIndex"</span>: 5535,
      <span class="s2">"ModifyIndex"</span>: 97009
    <span class="o">}</span>,
    <span class="s2">"Checks"</span>: <span class="o">[</span>
      <span class="o">{</span>
        <span class="s2">"Node"</span>: <span class="s2">"puppetmaster.example.lan"</span>,
        <span class="s2">"CheckID"</span>: <span class="s2">"puppetmaster_tcp"</span>,
        <span class="s2">"Name"</span>: <span class="s2">"puppetmaster_tcp"</span>,
        <span class="s2">"Status"</span>: <span class="s2">"critical"</span>,
        <span class="s2">"Notes"</span>: <span class="s2">"Puppetmasters listen on port 8140"</span>,
        <span class="s2">"Output"</span>: <span class="s2">"dial tcp [::1]:8140: getsockopt: connection refused"</span>,
        <span class="s2">"ServiceID"</span>: <span class="s2">"puppetmaster"</span>,
        <span class="s2">"ServiceName"</span>: <span class="s2">"puppetmaster"</span>,
        <span class="s2">"CreateIndex"</span>: 5601,
        <span class="s2">"ModifyIndex"</span>: 97009
      <span class="o">}</span>,
      <span class="o">{</span>
        <span class="s2">"Node"</span>: <span class="s2">"puppetmaster.example.lan"</span>,
        <span class="s2">"CheckID"</span>: <span class="s2">"serfHealth"</span>,
        <span class="s2">"Name"</span>: <span class="s2">"Serf Health Status"</span>,
        <span class="s2">"Status"</span>: <span class="s2">"passing"</span>,
        <span class="s2">"Notes"</span>: <span class="s2">""</span>,
        <span class="s2">"Output"</span>: <span class="s2">"Agent alive and reachable"</span>,
        <span class="s2">"ServiceID"</span>: <span class="s2">""</span>,
        <span class="s2">"ServiceName"</span>: <span class="s2">""</span>,
        <span class="s2">"CreateIndex"</span>: 5,
        <span class="s2">"ModifyIndex"</span>: 11150
      <span class="o">}</span>
    <span class="o">]</span>
  <span class="o">}</span>
<span class="o">]</span></code></pre></figure>

<p>Note here how the service is returning critical, so consul has removed it from the DNS query! Easy!</p>

<p>Now if I start it back up, it will of course start serving traffic again and become available in the DNS query:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>root@puppetmaster ~]# service httpd start
Redirecting to /bin/systemctl start  httpd.service
<span class="o">[</span>root@puppetmaster ~]# dig +short @127.0.0.1 <span class="nt">-p</span> 8600 puppetmaster.service.home.consul. SRV
1 1 8140 puppetmaster.example.lan.node.home.consul.</code></pre></figure>

<h3 id="dns-resolution">DNS resolution</h3>

<p>The final piece of this puzzle is to make sure regular DNS traffic can perform these queries. Because consul serves DNS on a non-standard port, we need to figure out how standard DNS queries from applications that expect DNS to always be on port 53 can get in on the action. There are a couple of ways of doing this:</p>

<ul>
  <li>Have your DNS servers forward queries for the .consul domain to their local agent</li>
  <li>Install a stub resolver or caching resolver on each host which does support port config, like dnsmasq.</li>
</ul>

<p>In my homelab, I went for option 2, but I would imagine in lots of production environments this wouldn’t really be an options, so forwarding with bind would be a better idea. Your mileage may vary.</p>

<h4 id="configuring-dnsmasq">Configuring DNSmasq</h4>

<p>Assuming dnsmasq is installed, you just need a config option in /etc/dnsmasq.d/10-consul like so:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">server</span><span class="o">=</span>/consul/127.0.0.1#8600</code></pre></figure>

<p>Now, set your resolv.conf to look at localhost first:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">nameserver 127.0.0.1</code></pre></figure>

<p>And now you can make DNS queries without the port for consul services!</p>

<h3 id="puppetmaster-service-deployment">Puppetmaster Service Deployment</h3>

<p>For the final step, you need to do a final thing for your Puppermasters. Because the puppetmaster is now being served on the address puppetmaster.service.home.consul, you’ll need to tweak your puppet config slightly to get things working. First, updated the cert names allowed adding the following to your master’s /etc/puppet/puppet.conf:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">dns_alt_names</span><span class="o">=</span>puppetmaster.service.home.consul</code></pre></figure>

<p>Then, clean our the master’s <em>client</em> key (not the CA!) and regenerate a new cert:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/puppet/ssl/private_keys/puppetmaster.example.lan.pem
puppet cert clean puppetmaster.example.lan
service httpd stop
puppet master <span class="nt">--no-daemonize</span></code></pre></figure>

<p>At this point, we should be able to run puppet against this new DNS name:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@puppetmaster ~]# puppet agent <span class="nt">-t</span> <span class="nt">--server</span><span class="o">=</span>puppetmaster.service.home.consul
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Loading facts
....</code></pre></figure>

<p>Now, we just need to change the master setting in our puppet.conf, which you can do with Puppet itself of course!</p>

<p>Congratulations, you’ve deployed a service with infrastructure service discovery!</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>What we’ve deployed here only used a single node, but the real benefits should be obvious.</p>

<ul>
  <li>When we deploy ANY new puppetmaster now, it will use our role definition, and automatically add the service and the healthcheck</li>
  <li>Using whatever deployment automation we use, we should be able to deploy a new service immediately and it will automatically start serving traffic for our infrastructure - no additional config necessary</li>
</ul>

<p>This article only covers a few of the possibilites with consul. I didn’t cover the <a href="https://www.consul.io/intro/getting-started/kv.html">key/value store</a> or adding services dynamically <a href="https://www.consul.io/intro/getting-started/services.html">using the API</a>. Consul also has first class support for distributed datacenters which wasn’t covered here, which means you can even distribute your services across DC’s and over the WAN.</p>

<br>

<div id="disqus_thread"></div>
<script>


var disqus_config = function () {
this.page.url = 'http://localhost:4000/assets/style.css';
this.page.identifier = '/consul/2016/02/08/infrastructure-service-discovery.html'; 
};

(function() {
var d = document, s = d.createElement('script');
s.src = 'https://.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<br>
<div class="about">
<div class="about__devider">*****</div>
<div class="about__text">
<br>
&#169 2021, Lee Briggs | <a href="https://github.com/ritijjain/pudhina-fresh">Pudhina Fresh</a> theme for Jekyll.
</div>
</div>

</div>
</body>
</html>